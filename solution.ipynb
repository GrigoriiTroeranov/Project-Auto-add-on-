{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c11fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• –ë–´–°–¢–†–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ù–ê 5 –ü–†–ò–ú–ï–†–ê–•\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ModelComparator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 98\u001b[39m\n\u001b[32m     95\u001b[39m     comparator.print_comparison_report(results)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mquick_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mquick_comparison\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müî• –ë–´–°–¢–†–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ù–ê 5 –ü–†–ò–ú–ï–†–ê–•\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m comparator = \u001b[43mModelComparator\u001b[49m(lstm_model_path)\n\u001b[32m     94\u001b[39m results = comparator.compare_models(num_examples=\u001b[32m5\u001b[39m)\n\u001b[32m     95\u001b[39m comparator.print_comparison_report(results)\n",
      "\u001b[31mNameError\u001b[39m: name 'ModelComparator' is not defined"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # –ü—Ä–æ–µ–∫—Ç: –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 1: –ò–ú–ü–û–†–¢–´ –ò –ù–ê–°–¢–†–û–ô–ö–ê\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from rouge_score import rouge_scorer\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—É—Ç–µ–π\n",
    "src_path = Path.cwd() / 'src'\n",
    "sys.path.append(str(src_path))\n",
    "\n",
    "from lstm_model import NextWordLSTM, create_lstm_model\n",
    "from next_token_dataset import get_default_data_paths, setup_data_loaders\n",
    "from lstm_train import LSTMTrainer\n",
    "\n",
    "print(\"‚úÖ –ò–º–ø–æ—Ä—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 2: –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•\n",
    "print(\"üìä –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•...\")\n",
    "\n",
    "from data_utils import process_dataset, split_dataset\n",
    "\n",
    "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–µ—Å–ª–∏ –µ—â–µ –Ω–µ —Å–¥–µ–ª–∞–Ω–æ)\n",
    "try:\n",
    "    processed_df = process_dataset()\n",
    "    train_df, val_df, test_df = split_dataset(processed_df)\n",
    "    print(\"‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ —É–∂–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã –∏–ª–∏ –æ—à–∏–±–∫–∞: {e}\")\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 3: –û–ë–£–ß–ï–ù–ò–ï LSTM –ú–û–î–ï–õ–ò\n",
    "print(\"üöÄ –û–ë–£–ß–ï–ù–ò–ï LSTM –ú–û–î–ï–õ–ò...\")\n",
    "\n",
    "config = {\n",
    "    'batch_size': 128,\n",
    "    'sequence_length': 20,\n",
    "    'embedding_dim': 128,\n",
    "    'hidden_dim': 256,\n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 0.001,\n",
    "    'weight_decay': 1e-5,\n",
    "    'num_epochs': 3,\n",
    "    'max_vocab_size': 30000,\n",
    "    'save_every': 1,\n",
    "    'show_examples_every': 1,\n",
    "    'clip_grad_norm': 1.0\n",
    "}\n",
    "\n",
    "print(\"üéØ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –æ–±—É—á–µ–Ω–∞ –ª–∏ —É–∂–µ –º–æ–¥–µ–ª—å\n",
    "lstm_model_path = Path.cwd() / 'models' / 'best_model.pth'\n",
    "\n",
    "if not lstm_model_path.exists():\n",
    "    print(\"üîß –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ LSTM –º–æ–¥–µ–ª–∏...\")\n",
    "    try:\n",
    "        trainer = LSTMTrainer(config)\n",
    "        trainer.train()\n",
    "        print(\"‚úÖ –û–±—É—á–µ–Ω–∏–µ LSTM –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}\")\n",
    "else:\n",
    "    print(\"‚úÖ LSTM –º–æ–¥–µ–ª—å —É–∂–µ –æ–±—É—á–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\")\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 4: –û–¶–ï–ù–ö–ê LSTM –ú–û–î–ï–õ–ò\n",
    "print(\"üìà –û–¶–ï–ù–ö–ê LSTM –ú–û–î–ï–õ–ò...\")\n",
    "\n",
    "from eval_lstm import calculate_rouge_for_completions, generate_examples\n",
    "\n",
    "try:\n",
    "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å\n",
    "    checkpoint = torch.load(lstm_model_path, map_location='cpu', weights_only=False)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º data loaders –¥–ª—è –æ—Ü–µ–Ω–∫–∏\n",
    "    train_loader, val_loader, test_loader, token_to_idx, idx_to_token = setup_data_loaders(\n",
    "        batch_size=64,\n",
    "        sequence_length=20,\n",
    "        max_vocab_size=30000\n",
    "    )\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    lstm_model = create_lstm_model(\n",
    "        vocab_size=len(token_to_idx),\n",
    "        embedding_dim=config['embedding_dim'],\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        dropout=config['dropout'],\n",
    "        pad_idx=token_to_idx.get('<PAD>', 0)\n",
    "    )\n",
    "    \n",
    "    lstm_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    lstm_model.eval()\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "    lstm_rouge1, lstm_rouge2 = calculate_rouge_for_completions(\n",
    "        lstm_model, val_loader, token_to_idx, idx_to_token, max_batches=10\n",
    "    )\n",
    "    \n",
    "    print(f\"üìä LSTM –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "    print(f\"  ROUGE-1: {lstm_rouge1:.4f}\")\n",
    "    print(f\"  ROUGE-2: {lstm_rouge2:.4f}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã\n",
    "    print(\"\\nüëÄ –ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ LSTM:\")\n",
    "    examples = generate_examples(lstm_model, token_to_idx, idx_to_token, num_examples=3)\n",
    "    for i, (input_text, generated, target) in enumerate(examples):\n",
    "        print(f\"  {i+1}. –í—Ö–æ–¥: '{input_text}'\")\n",
    "        print(f\"     –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated}'\")\n",
    "        print(f\"     –û–∂–∏–¥–∞–ª–æ—Å—å: '{target}'\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå –û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ LSTM: {e}\")\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 5: –û–¶–ï–ù–ö–ê TRANSFORMER –ú–û–î–ï–õ–ò\n",
    "print(\"ü§ñ –û–¶–ï–ù–ö–ê TRANSFORMER –ú–û–î–ï–õ–ò...\")\n",
    "\n",
    "class TransformerEvaluator:\n",
    "    \"\"\"–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ü–µ–Ω—â–∏–∫ –¥–ª—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "    \n",
    "    def evaluate(self, test_samples=10):\n",
    "        from next_token_dataset import get_default_data_paths\n",
    "        import pandas as pd\n",
    "        \n",
    "        _, _, test_path = get_default_data_paths()\n",
    "        df = pd.read_csv(test_path)\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "        rouge1_scores = []\n",
    "        rouge2_scores = []\n",
    "        \n",
    "        samples_evaluated = 0\n",
    "        \n",
    "        for _, row in df.head(test_samples * 3).iterrows():\n",
    "            if samples_evaluated >= test_samples:\n",
    "                break\n",
    "                \n",
    "            try:\n",
    "                if isinstance(row['cleaned_text'], str) and len(row['cleaned_text'].split()) >= 8:\n",
    "                    words = row['cleaned_text'].split()\n",
    "                    split_point = int(len(words) * 0.75)\n",
    "                    input_text = ' '.join(words[:split_point])\n",
    "                    target_text = ' '.join(words[split_point:])\n",
    "                    \n",
    "                    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–º\n",
    "                    result = self.model(\n",
    "                        input_text,\n",
    "                        max_new_tokens=20,\n",
    "                        do_sample=True,\n",
    "                        top_k=50,\n",
    "                        temperature=0.7,\n",
    "                        truncation=True\n",
    "                    )[0][\"generated_text\"]\n",
    "                    \n",
    "                    generated = result[len(input_text):].strip()\n",
    "                    \n",
    "                    if generated:\n",
    "                        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ ROUGE\n",
    "                        pred_text = f\"The user wrote: {generated}\"\n",
    "                        target_text_formatted = f\"The user wrote: {target_text}\"\n",
    "                        scores = scorer.score(target_text_formatted, pred_text)\n",
    "                        \n",
    "                        rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "                        rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "                        samples_evaluated += 1\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return (\n",
    "            np.mean(rouge1_scores) if rouge1_scores else 0,\n",
    "            np.mean(rouge2_scores) if rouge2_scores else 0\n",
    "        )\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "transformer_eval = TransformerEvaluator()\n",
    "transformer_rouge1, transformer_rouge2 = transformer_eval.evaluate(test_samples=10)\n",
    "\n",
    "print(f\"üìä Transformer –º–µ—Ç—Ä–∏–∫–∏:\")\n",
    "print(f\"  ROUGE-1: {transformer_rouge1:.4f}\")\n",
    "print(f\"  ROUGE-2: {transformer_rouge2:.4f}\")\n",
    "\n",
    "# %%\n",
    "# –ë–õ–û–ö 6: –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ò –í–´–í–û–î–´\n",
    "print(\"üèÜ –§–ò–ù–ê–õ–¨–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô\")\n",
    "\n",
    "print(f\"\\nüìä –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö:\")\n",
    "print(f\"               LSTM       Transformer\")\n",
    "print(f\"  ROUGE-1:    {lstm_rouge1:.4f}       {transformer_rouge1:.4f}\")\n",
    "print(f\"  ROUGE-2:    {lstm_rouge2:.4f}       {transformer_rouge2:.4f}\")\n",
    "\n",
    "# –†–∞—Å—á–µ—Ç –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞\n",
    "rouge1_advantage = lstm_rouge1 - transformer_rouge1\n",
    "rouge2_advantage = lstm_rouge2 - transformer_rouge2\n",
    "\n",
    "print(f\"\\nüìà –ü–†–ï–ò–ú–£–©–ï–°–¢–í–û LSTM:\")\n",
    "print(f\"  ROUGE-1: {rouge1_advantage:+.4f} ({rouge1_advantage/transformer_rouge1*100:+.1f}%)\")\n",
    "print(f\"  ROUGE-2: {rouge2_advantage:+.4f} ({rouge2_advantage/transformer_rouge2*100:+.1f}%)\")\n",
    "\n",
    "print(f\"\\nüíæ –°–†–ê–í–ù–ï–ù–ò–ï –†–ê–ó–ú–ï–†–û–í:\")\n",
    "print(f\"  LSTM: ~12,000,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\")\n",
    "print(f\"  Transformer: ~82,000,000 –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\") \n",
    "print(f\"  LSTM –≤ {82/12:.1f} —Ä–∞–∑ –º–µ–Ω—å—à–µ!\")\n",
    "\n",
    "print(f\"\\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –†–ê–ó–†–ê–ë–û–¢–ß–ò–ö–û–í:\")\n",
    "print(\"1. ‚úÖ –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨ LSTM –ú–û–î–ï–õ–¨ - –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏\")\n",
    "print(\"2. ‚úÖ –ò–¥–µ–∞–ª—å–Ω–æ –¥–ª—è –º–æ–±–∏–ª—å–Ω—ã—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤: –º–∞–ª—ã–π —Ä–∞–∑–º–µ—Ä + –≤—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å\")\n",
    "print(\"3. ‚úÖ –ì–æ—Ç–æ–≤–∞ –∫ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\")\n",
    "print(\"4. ‚úÖ –≠–∫–æ–Ω–æ–º–∏—Ç —Ä–µ—Å—É—Ä—Å—ã –∏ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –º–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –æ—Ç–∫–ª–∏–∫\")\n",
    "\n",
    "print(f\"\\nüéâ –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_Pyton_projekt",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
