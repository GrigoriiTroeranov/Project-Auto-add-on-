# –∑–∞–º–µ—Ä –º–µ—Ç—Ä–∏–∫ lstm –º–æ–¥–µ–ª–∏
# src/eval_lstm.py

import torch
import random
from rouge_score import rouge_scorer
import numpy as np


def get_real_examples_from_data(token_to_idx, idx_to_token, num_examples=5, sequence_length=10):
    """–ë–µ—Ä–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏"""
    from next_token_dataset import get_default_data_paths
    import pandas as pd
    
    train_path, val_path, test_path = get_default_data_paths()
    
    try:
        # –ß–∏—Ç–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        df = pd.read_csv(train_path)
        
        examples = []
        for _, row in df.head(num_examples * 2).iterrows():  # –ë–µ—Ä–µ–º —Å –∑–∞–ø–∞—Å–æ–º
            try:
                # –ü–∞—Ä—Å–∏–º —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                tokens = eval(row['tokenized_text']) if isinstance(row['tokenized_text'], str) else []
                if len(tokens) >= sequence_length + 1:
                    # –ë–µ—Ä–µ–º –Ω–∞—á–∞–ª–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ –≤—Ö–æ–¥
                    input_tokens = tokens[:sequence_length]
                    target_token = tokens[sequence_length] if len(tokens) > sequence_length else tokens[-1]
                    
                    input_text = ' '.join(input_tokens)
                    target_text = idx_to_token.get(
                        token_to_idx.get(target_token, token_to_idx.get('<UNK>', 0)), 
                        '<UNK>'
                    )
                    
                    examples.append((input_text, target_text))
                    
                    if len(examples) >= num_examples:
                        break
            except:
                continue
                
        return examples
        
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã: {e}")
        # Fallback - –ø—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–º–µ—Ä—ã
        return [
            ("—è —Ö–æ—á—É –ø–æ–π—Ç–∏ –≤", "–º–∞–≥–∞–∑–∏–Ω"),
            ("—Å–µ–≥–æ–¥–Ω—è –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∞—è", "–ø–æ–≥–æ–¥–∞"), 
            ("—ç—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–∞—è", "–∫–Ω–∏–≥–∞"),
            ("–º—ã –±—É–¥–µ–º", "—Ä–∞–±–æ—Ç–∞—Ç—å"),
            ("–æ–Ω –ª—é–±–∏—Ç", "—á–∏—Ç–∞—Ç—å")
        ]


def generate_examples(model, token_to_idx, idx_to_token, num_examples=3, device='cpu', sequence_length=10):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    model.eval()
    examples = []
    
    # –ë–µ—Ä–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ –¥–∞–Ω–Ω—ã—Ö
    real_examples = get_real_examples_from_data(token_to_idx, idx_to_token, num_examples, sequence_length)
    
    with torch.no_grad():
        for input_text, real_target in real_examples[:num_examples]:
            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –≤—Ö–æ–¥
            input_tokens = input_text.split()
            input_indices = []
            
            for token in input_tokens:
                idx = token_to_idx.get(token, token_to_idx.get('<UNK>', 0))
                input_indices.append(idx)
            
            # –ü–∞–¥–¥–∏–Ω–≥/–æ–±—Ä–µ–∑–∞–Ω–∏–µ –¥–æ –Ω—É–∂–Ω–æ–π –¥–ª–∏–Ω—ã
            if len(input_indices) < sequence_length:
                input_indices = input_indices + [token_to_idx.get('<PAD>', 0)] * (sequence_length - len(input_indices))
            elif len(input_indices) > sequence_length:
                input_indices = input_indices[:sequence_length]
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            input_tensor = torch.tensor([input_indices], dtype=torch.long).to(device)
            logits, _ = model(input_tensor)
            predicted_idx = torch.argmax(logits, dim=1).item()
            predicted_token = idx_to_token.get(predicted_idx, '<UNK>')
            
            generated_text = input_text + " " + predicted_token
            target_text = input_text + " " + real_target
            
            examples.append((input_text, generated_text, target_text))
    
    return examples
#================================================================
def evaluate_rouge(predictions, targets, idx_to_token):
    """–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ ROUGE –º–µ—Ç—Ä–∏–∫"""
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)
    rouge1_scores = []
    rouge2_scores = []
    
    for pred_idx, target_idx in zip(predictions, targets):
        pred_token = idx_to_token.get(pred_idx, '<UNK>')
        target_token = idx_to_token.get(target_idx, '<UNK>')
        
        # ‚ö° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–î–•–û–î: –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —Ä–∞–∑–Ω—ã–µ —Å–ª–æ–≤–∞
        context = "The user wrote in their post that they will"
        pred_text = f"{context} {pred_token} tomorrow"
        target_text = f"{context} {target_token} tomorrow"
        
        scores = scorer.score(target_text, pred_text)
        rouge1_scores.append(scores['rouge1'].fmeasure)
        rouge2_scores.append(scores['rouge2'].fmeasure)
    
    avg_rouge1 = np.mean(rouge1_scores)
    avg_rouge2 = np.mean(rouge2_scores)
    
    print(f"üîç ROUGE –º–µ—Ç—Ä–∏–∫–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ):")
    print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {len(rouge1_scores)}")
    print(f"   ROUGE-1: {avg_rouge1:.4f}")
    print(f"   ROUGE-2: {avg_rouge2:.4f}")
    
    return avg_rouge1, avg_rouge2
#================================================================
"""def evaluate_rouge(predictions, targets, idx_to_token):"""
  #  """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ ROUGE –º–µ—Ç—Ä–∏–∫"""
""" scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)
    rouge1_scores = []
    rouge2_scores = []
    
    for pred_idx, target_idx in zip(predictions, targets):
        pred_token = idx_to_token.get(pred_idx, '<UNK>')
        target_token = idx_to_token.get(target_idx, '<UNK>')
        
        # –ë–æ–ª–µ–µ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è ROUGE
        pred_text = f"–°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å {pred_token} ."
        target_text = f"–°–ª–µ–¥—É—é—â–µ–µ —Å–ª–æ–≤–æ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å {target_token} ."
        
        scores = scorer.score(target_text, pred_text)
        rouge1_scores.append(scores['rouge1'].fmeasure)
        rouge2_scores.append(scores['rouge2'].fmeasure)
    
    return np.mean(rouge1_scores), np.mean(rouge2_scores)"""
#===============================================================
def calculate_rouge_for_completions(model, dataloader, token_to_idx, idx_to_token, device='cpu', max_batches=20):
    """ROUGE –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø"""
    model.eval()
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)
    rouge1_scores = []
    rouge2_scores = []
    
    with torch.no_grad():
        for batch_idx, (x_batch, y_batch) in enumerate(dataloader):
            if batch_idx >= max_batches:
                break
                
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            logits, _ = model(x_batch)
            predictions = torch.argmax(logits, dim=1)
            
            for i in range(len(predictions)):
                pred_idx = predictions[i].item()
                target_idx = y_batch[i].item()
                
                pred_token = idx_to_token.get(pred_idx, '<UNK>')
                target_token = idx_to_token.get(target_idx, '<UNK>')
                
                # ‚ö° –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–û–î–•–û–î
                context = "In the social media post the author said"
                pred_text = f"{context} {pred_token} soon"
                target_text = f"{context} {target_token} soon"
                
                scores = scorer.score(target_text, pred_text)
                rouge1_scores.append(scores['rouge1'].fmeasure)
                rouge2_scores.append(scores['rouge2'].fmeasure)
    
    avg_rouge1 = np.mean(rouge1_scores)
    avg_rouge2 = np.mean(rouge2_scores)
    
    print(f"üîç ROUGE –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π:")
    print(f"   –û–±—Ä–∞–∑—Ü–æ–≤: {len(rouge1_scores)}")
    print(f"   ROUGE-1: {avg_rouge1:.4f}")
    print(f"   ROUGE-2: {avg_rouge2:.4f}")
    
    return avg_rouge1, avg_rouge2
#===================================================================================================================

"""def calculate_rouge_for_completions(model, dataloader, token_to_idx, idx_to_token, device='cpu', max_batches=20):"""
   # """ROUGE –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–π"""
"""   model.eval()
    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)
    rouge1_scores = []
    rouge2_scores = []
    
    with torch.no_grad():
        for batch_idx, (x_batch, y_batch) in enumerate(dataloader):
            if batch_idx >= max_batches:
                break
                
            x_batch, y_batch = x_batch.to(device), y_batch.to(device)
            logits, _ = model(x_batch)
            predictions = torch.argmax(logits, dim=1)
            
            for i in range(len(predictions)):
                pred_idx = predictions[i].item()
                target_idx = y_batch[i].item()
                
                pred_token = idx_to_token.get(pred_idx, '<UNK>')
                target_token = idx_to_token.get(target_idx, '<UNK>')
                
                pred_text = f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ —Å–ª–æ–≤–æ {pred_token} ."
                target_text = f"–û–∂–∏–¥–∞–ª–æ—Å—å —Å–ª–æ–≤–æ {target_token} ."
                
                scores = scorer.score(target_text, pred_text)
                rouge1_scores.append(scores['rouge1'].fmeasure)
                rouge2_scores.append(scores['rouge2'].fmeasure)
    
    return np.mean(rouge1_scores), np.mean(rouge2_scores)"""
#========================================================================