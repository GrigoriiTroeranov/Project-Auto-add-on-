# src/lstm_train.py

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from pathlib import Path
from tqdm import tqdm
import numpy as np

from lstm_model import create_lstm_model, NextWordLSTM
from next_token_dataset import setup_data_loaders
from eval_lstm import calculate_rouge_for_completions, generate_examples


class LSTMTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è LSTM –º–æ–¥–µ–ª–∏ —Å –ø–æ–ª–Ω—ã–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –ø—Ä–æ–º—Ç–∞
    """
    
    def __init__(self, config):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏
        self.setup_data_and_model()
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=config['learning_rate'],
            weight_decay=config.get('weight_decay', 1e-5)
        )
        self.criterion = nn.CrossEntropyLoss()
        
        self.train_losses = []
        self.val_losses = []
        self.rouge1_scores = []
        self.rouge2_scores = []
   
#=======================================================  #"""–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏"""
    """  
    def setup_data_and_model(self):
       
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        (self.train_loader, self.val_loader, self.test_loader, 
         self.token_to_idx, self.idx_to_token) = setup_data_loaders(
            batch_size=self.config['batch_size'],
            sequence_length=self.config['sequence_length']
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = create_lstm_model(
            vocab_size=len(self.token_to_idx),
            embedding_dim=self.config['embedding_dim'],
            hidden_dim=self.config['hidden_dim'],
            num_layers=self.config['num_layers'],
            dropout=self.config['dropout'],
            pad_idx=self.token_to_idx.get('<PAD>', 0)
        ).to(self.device)
        
        print(f"–ú–æ–¥–µ–ª—å: {sum(p.numel() for p in self.model.parameters()):,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")"""

 #====================================================== """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º"""
    def setup_data_and_model(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –º–æ–¥–µ–ª–∏"""
        from next_token_dataset import setup_data_loaders
    
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –° –û–ì–†–ê–ù–ò–ß–ï–ù–ù–´–ú –°–õ–û–í–ê–†–ï–ú
        (self.train_loader, self.val_loader, self.test_loader, 
        self.token_to_idx, self.idx_to_token) = setup_data_loaders(
            batch_size=self.config['batch_size'],
            sequence_length=self.config['sequence_length'],
            max_vocab_size=self.config.get('max_vocab_size', 30000)  # ‚¨ÖÔ∏è –î–û–ë–ê–í–¨–¢–ï –≠–¢–û 
        )
    
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        self.model = create_lstm_model(
            vocab_size=len(self.token_to_idx),
            embedding_dim=self.config['embedding_dim'],
            hidden_dim=self.config['hidden_dim'],
            num_layers=self.config['num_layers'],
            dropout=self.config['dropout'],
            pad_idx=self.token_to_idx.get('<PAD>', 0)
        ).to(self.device)
    
        print(f"–ú–æ–¥–µ–ª—å: {sum(p.numel() for p in self.model.parameters()):,} –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
        print(f"–†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è: {len(self.token_to_idx):,} —Ç–æ–∫–µ–Ω–æ–≤")
    
 # ===================================================== 
    def train_epoch(self, max_batches=None):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–µ"""
        self.model.train()
        total_loss = 0
        batch_count = 0
        
        progress_bar = tqdm(self.train_loader, desc="Training")
        for batch_idx, (x_batch, y_batch) in enumerate(progress_bar):
            if max_batches and batch_idx >= max_batches:
                break
                
            x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)
            
            # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
            self.optimizer.zero_grad()
            logits, _ = self.model(x_batch)
            loss = self.criterion(logits, y_batch)
            
            # –û–±—Ä–∞—Ç–Ω—ã–π –ø—Ä–æ—Ö–æ–¥
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            batch_count += 1
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞
            if batch_idx % 10 == 0:
                progress_bar.set_postfix({'Loss': f'{loss.item():.4f}'})
        
        avg_loss = total_loss / batch_count
        self.train_losses.append(avg_loss)
        return avg_loss
    
    def validate_with_rouge(self, max_batches=None):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ–º ROUGE –º–µ—Ç—Ä–∏–∫"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        batch_count = 0
        
        all_predictions = []
        all_targets = []
        
        with torch.no_grad():
            for batch_idx, (x_batch, y_batch) in enumerate(self.val_loader):
                if max_batches and batch_idx >= max_batches:
                    break
                    
                x_batch, y_batch = x_batch.to(self.device), y_batch.to(self.device)
                logits, _ = self.model(x_batch)
                loss = self.criterion(logits, y_batch)
                total_loss += loss.item()
                
                # –î–ª—è accuracy
                preds = torch.argmax(logits, dim=1)
                correct += (preds == y_batch).sum().item()
                total += y_batch.size(0)
                batch_count += 1
                
                # –î–ª—è ROUGE
                all_predictions.extend(preds.cpu().numpy())
                all_targets.extend(y_batch.cpu().numpy())
        
        avg_loss = total_loss / batch_count
        accuracy = correct / total if total > 0 else 0
        self.val_losses.append(avg_loss)
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ ROUGE –º–µ—Ç—Ä–∏–∫
        from eval_lstm import evaluate_rouge
        rouge1, rouge2 = evaluate_rouge(all_predictions, all_targets, self.idx_to_token)
        self.rouge1_scores.append(rouge1)
        self.rouge2_scores.append(rouge2)
        
        return avg_loss, accuracy, rouge1, rouge2
    
    def train(self, max_batches_per_epoch=None):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å ROUGE –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏"""
        print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è LSTM –º–æ–¥–µ–ª–∏...")
        best_rouge1 = 0
        
        for epoch in range(self.config['num_epochs']):
            print(f"\n–≠–ø–æ—Ö–∞ {epoch + 1}/{self.config['num_epochs']}")
            print("-" * 50)
            
            # –û–±—É—á–µ–Ω–∏–µ
            train_loss = self.train_epoch(max_batches=max_batches_per_epoch)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å ROUGE
            val_loss, val_acc, rouge1, rouge2 = self.validate_with_rouge(max_batches=50)
            
            # –í—ã–≤–æ–¥ –º–µ—Ç—Ä–∏–∫
            print(f"Train Loss: {train_loss:.4f}")
            print(f"Val Loss: {val_loss:.4f}") 
            print(f"Val Accuracy: {val_acc:.2%}")
            print(f"ROUGE-1: {rouge1:.4f}")
            print(f"ROUGE-2: {rouge2:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            if rouge1 > best_rouge1:
                best_rouge1 = rouge1
                self.save_model('best_model.pth')
                print(f"üöÄ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å! ROUGE-1: {rouge1:.4f}")
            
            # –ü–æ–∫–∞–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ –∫–∞–∂–¥—ã–µ N —ç–ø–æ—Ö
            if (epoch + 1) % self.config.get('show_examples_every', 2) == 0:
                print("\n–ü—Ä–∏–º–µ—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
                examples = generate_examples(
                    self.model, 
                    self.token_to_idx, 
                    self.idx_to_token,
                    num_examples=2,
                    device=self.device
                )
                for i, (input_text, generated, target) in enumerate(examples):
                    print(f"  {i+1}. –í—Ö–æ–¥: '{input_text}'")
                    print(f"     –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: '{generated}'")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
            if (epoch + 1) % self.config.get('save_every', 1) == 0:
                self.save_model(f'checkpoint_epoch_{epoch+1}.pth')
        
        # –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        self.save_model('final_model.pth')
        print("\nüéâ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –ò—Ç–æ–≥–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        print(f"\n–õ—É—á—à–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
        print(f"  ROUGE-1: {max(self.rouge1_scores):.4f}")
        print(f"  ROUGE-2: {max(self.rouge2_scores):.4f}")
    
    def save_model(self, filename):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏"""
        models_dir = Path(__file__).parent.parent / 'models'
        models_dir.mkdir(exist_ok=True)
        
        model_path = models_dir / filename
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'token_to_idx': self.token_to_idx,
            'idx_to_token': self.idx_to_token,
            'config': self.config,
            'train_losses': self.train_losses,
            'val_losses': self.val_losses,
            'rouge1_scores': self.rouge1_scores,
            'rouge2_scores': self.rouge2_scores
        }, model_path)
        
        print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")


# –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ –ø—Ä–æ–º—Ç–∞
def get_optimal_config():
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø—Ä–æ–º—Ç–∞"""
    return {
        'batch_size': 256,
        'sequence_length': 20,
        'embedding_dim': 128,
        'hidden_dim': 256,  # –ú–æ–∂–Ω–æ —É–≤–µ–ª–∏—á–∏—Ç—å –¥–æ 512 –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏
        'num_layers': 2,
        'dropout': 0.3,
        'learning_rate': 0.001,
        'weight_decay': 1e-5,
        'num_epochs': 10,
        'save_every': 2,
        'show_examples_every': 2,
        'early_stopping_patience': 3
    }


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    config = get_optimal_config()
    
    print("üéØ –û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    
    trainer = LSTMTrainer(config)
    trainer.train()  # –ü–æ–ª–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π


if __name__ == "__main__":
    main()